{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765299a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib_run_single\n",
    "from desktop_env.desktop_env import DesktopEnv\n",
    "from gui_agents.s3.agents.agent_s import AgentS3\n",
    "from gui_agents.s3.agents.grounding import OSWorldACI\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5704b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # environment config\n",
    "    path_to_vm=None,\n",
    "    provider_name=\"vmware\",          # <--- not vmware\n",
    "    headless=True,\n",
    "    action_space=\"pyautogui\",\n",
    "    observation_type=\"screenshot_a11y_tree\",\n",
    "    screen_width=1920,\n",
    "    screen_height=1080,\n",
    "    sleep_after_execution=3.0,\n",
    "    max_steps=5,                     # small for debugging\n",
    "\n",
    "    # agent config\n",
    "    max_trajectory_length=3,\n",
    "    test_config_base_dir=\"evaluation_examples\",\n",
    "\n",
    "    # main LM config (Ollama llama3.2:3b)\n",
    "    model=\"qwen2.5vl:32b\",\n",
    "    temperature=0.0,\n",
    "    model_provider=\"openai\",         # Agent-S uses OpenAI-style client\n",
    "    model_url=\"http://localhost:11434/v1\",\n",
    "    model_api_key=\"ollama\",\n",
    "    model_temperature=0.0,\n",
    "\n",
    "    # grounding model config (reuse same endpoint/model for now)\n",
    "    ground_provider=\"openai\",\n",
    "    ground_url=\"http://localhost:1234/v1\",\n",
    "    ground_api_key=\"lm-studio\",\n",
    "    ground_model=\"ui-tars-7b-dpo@q4_k_s\",\n",
    "    grounding_width=1000,\n",
    "    grounding_height=1000,\n",
    "\n",
    "    # example config\n",
    "    domain=\"all\",\n",
    "    test_all_meta_path=\"evaluation_examples/test_all.json\",\n",
    "\n",
    "    # logging / results\n",
    "    result_dir=\"./results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798b0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build engine params\n",
    "engine_params = {\n",
    "    \"engine_type\": args.model_provider,\n",
    "    \"model\": args.model,\n",
    "    \"base_url\": getattr(args, \"model_url\", \"\"),\n",
    "    \"api_key\": getattr(args, \"model_api_key\", \"\"),\n",
    "    \"temperature\": getattr(args, \"model_temperature\", None),\n",
    "}\n",
    "\n",
    "engine_params_for_grounding = {\n",
    "    \"engine_type\": args.ground_provider,\n",
    "    \"model\": args.ground_model,\n",
    "    \"base_url\": getattr(args, \"ground_url\", \"\"),\n",
    "    \"api_key\": getattr(args, \"ground_api_key\", \"\"),\n",
    "    \"grounding_width\": args.grounding_width,\n",
    "    \"grounding_height\": args.grounding_height,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e05308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VMware VM...\n"
     ]
    }
   ],
   "source": [
    "# OSWorld environment (Docker provider)\n",
    "env = DesktopEnv(\n",
    "    provider_name=args.provider_name,\n",
    "    path_to_vm=args.path_to_vm,\n",
    "    action_space=args.action_space,\n",
    "    screen_size=(args.screen_width, args.screen_height),\n",
    "    headless=args.headless,\n",
    "    os_type=\"Ubuntu\",\n",
    "    require_a11y_tree=args.observation_type in [\"a11y_tree\", \"screenshot_a11y_tree\", \"som\"],\n",
    "    enable_proxy=False,   # <--- TURNED OFF for now\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa179d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent-S grounding agent\n",
    "grounding_agent = OSWorldACI(\n",
    "    env=env,\n",
    "    platform=\"linux\",\n",
    "    engine_params_for_generation=engine_params,\n",
    "    engine_params_for_grounding=engine_params_for_grounding,\n",
    "    width=args.screen_width,\n",
    "    height=args.screen_height,\n",
    ")\n",
    "\n",
    "# Agent-S3 main agent\n",
    "agent = AgentS3(\n",
    "    engine_params,\n",
    "    grounding_agent,\n",
    "    platform=\"linux\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6399f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_for_path(name: str) -> str:\n",
    "    # Replace any char that is not letter, digit, underscore, dot, or dash\n",
    "    return re.sub(r'[^A-Za-z0-9_.-]', '_', name)\n",
    "\n",
    "safe_model_name = sanitize_for_path(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd7a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: chrome\n",
      "Example IDs: ['bb5e4c0d-f964-439c-97b6-bdb9747de3f4']\n",
      "\n",
      "==============================\n",
      "Running example: chrome/bb5e4c0d-f964-439c-97b6-bdb9747de3f4\n",
      "Config file path: evaluation_examples\\examples/chrome/bb5e4c0d-f964-439c-97b6-bdb9747de3f4.json\n",
      "Instruction: Can you make Bing the main search engine when I look stuff up on the internet?\n",
      "Result dir: ./results\\pyautogui\\screenshot_a11y_tree\\qwen2.5vl_32b\\chrome\\bb5e4c0d-f964-439c-97b6-bdb9747de3f4\n",
      "Starting VMware VM...\n",
      "Response success!\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(987,83)<|box_end|>\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(987,83)<|box_end|>\n",
      "Response success!\n",
      "Response success!\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(817,756)<|box_end|>\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(817,756)<|box_end|>\n",
      "Response success!\n",
      "Response success!\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(102,368)<|box_end|>\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(102,368)<|box_end|>\n",
      "Response success!\n",
      "Response success!\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(810,325)<|box_end|>\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(810,325)<|box_end|>\n",
      "Response success!\n",
      "Response success!\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(78,913)<|box_end|>\n",
      "Response success!\n",
      "RAW GROUNDING MODEL RESPONSE: <|box_start|>(78,913)<|box_end|>\n",
      "Google ['Microsoft Bing', 'Bing']\n",
      "Scores list: [0.0]\n",
      "Latency: 713.360 seconds\n",
      "Final score: 0.0\n",
      "Success: False\n",
      "\n",
      "===== Summary over first 10 tasks =====\n",
      "chrome/bb5e4c0d-f964-439c-97b6-bdb9747de3f4: score=0.0, success=False, latency=713.360s\n",
      "\n",
      "Overall accuracy: 0.0%\n",
      "Average latency: 713.360 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open(args.test_all_meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_all_meta = json.load(f)\n",
    "\n",
    "# Take the first domain\n",
    "domain = next(iter(test_all_meta.keys()))\n",
    "example_ids = test_all_meta[domain][:1]   # first 10 tasks in this domain\n",
    "\n",
    "print(\"Domain:\", domain)\n",
    "print(\"Example IDs:\", example_ids)\n",
    "\n",
    "safe_model_name = sanitize_for_path(args.model)\n",
    "\n",
    "results = []  # to store per-task stats\n",
    "\n",
    "for example_id in example_ids:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Running example: {domain}/{example_id}\")\n",
    "\n",
    "    config_file = os.path.join(\n",
    "        args.test_config_base_dir, f\"examples/{domain}/{example_id}.json\"\n",
    "    )\n",
    "    print(\"Config file path:\", config_file)\n",
    "\n",
    "    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        example = json.load(f)\n",
    "\n",
    "    print(\"Instruction:\", example[\"instruction\"])\n",
    "\n",
    "    # Result dir for this example\n",
    "    example_result_dir = os.path.join(\n",
    "        args.result_dir,\n",
    "        args.action_space,\n",
    "        args.observation_type,\n",
    "        safe_model_name,\n",
    "        domain,\n",
    "        example_id,\n",
    "    )\n",
    "    os.makedirs(example_result_dir, exist_ok=True)\n",
    "    print(\"Result dir:\", example_result_dir)\n",
    "\n",
    "    # For this single run, collect scores locally\n",
    "    scores: list[float] = []\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    lib_run_single.run_single_example(\n",
    "        agent,\n",
    "        env,\n",
    "        example,\n",
    "        args.max_steps,\n",
    "        example[\"instruction\"],\n",
    "        args,\n",
    "        example_result_dir,\n",
    "        scores,\n",
    "    )\n",
    "    end = time.perf_counter()\n",
    "    latency = end - start\n",
    "\n",
    "    print(\"Scores list:\", scores)\n",
    "\n",
    "    # Assume the last entry is the final score (adjust if your API differs)\n",
    "    final_score = scores[-1] if scores else None\n",
    "\n",
    "    # Define \"success\" for accuracy â€” adjust rule if needed\n",
    "    # Common pattern: score 1.0 = success, 0.0 = fail\n",
    "    success = None\n",
    "    if final_score is not None:\n",
    "        if final_score in (0, 1):\n",
    "            success = bool(final_score)\n",
    "        else:\n",
    "            # Fallback: treat positive score as success\n",
    "            success = final_score > 0\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"domain\": domain,\n",
    "            \"example_id\": example_id,\n",
    "            \"latency\": latency,\n",
    "            \"final_score\": final_score,\n",
    "            \"success\": success,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Latency: {latency:.3f} seconds\")\n",
    "    print(f\"Final score: {final_score}\")\n",
    "    print(f\"Success: {success}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# ---- Summary over the 10 tasks ----\n",
    "valid_success = [r[\"success\"] for r in results if r[\"success\"] is not None]\n",
    "accuracy = sum(valid_success) / len(valid_success) if valid_success else 0.0\n",
    "avg_latency = sum(r[\"latency\"] for r in results) / len(results)\n",
    "\n",
    "print(\"\\n===== Summary over first 10 tasks =====\")\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['domain']}/{r['example_id']}: \"\n",
    "        f\"score={r['final_score']}, success={r['success']}, latency={r['latency']:.3f}s\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nOverall accuracy: {accuracy * 100:.1f}%\")\n",
    "print(f\"Average latency: {avg_latency:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d89041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17547a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse291a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
