{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UGround vs Qwen2-VL Evaluation on Click-100k\n",
        "\n",
        "This notebook compares the performance of:\n",
        "- **osunlp/UGround-V1-2B**\n",
        "- **Qwen/Qwen2-VL-2B**\n",
        "\n",
        "on GUI element coordinate recognition using the first 100 samples from the `mlfoundations/Click-100k` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict, List\n",
        "import base64\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup SGLang API Clients\n",
        "\n",
        "**Note:** Before running this notebook, start the SGLang servers:\n",
        "```bash\n",
        "# Terminal 1: Serve UGround-V1-2B\n",
        "sgl serve osunlp/UGround-V1-2B --port 8000 --trust-remote-code\n",
        "\n",
        "# Terminal 2: Serve Qwen2-VL-2B  \n",
        "sgl serve Qwen/Qwen2-VL-2B --port 8001 --trust-remote-code\n",
        "```\n",
        "\n",
        "Alternatively, use the provided script:\n",
        "```bash\n",
        "python src/ecua/serve_models_sglang.py --model uground-2b --port 8000 --trust-remote-code\n",
        "python src/ecua/serve_models_sglang.py --model qwen2-vl-2b --port 8001 --trust-remote-code\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading models...\n",
            "Loading uground-2b (osunlp/UGround-V1-2B)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
            "/root/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ uground-2b loaded successfully\n",
            "Loading qwen2-vl-2b (Qwen/Qwen2-VL-2B)...\n",
            "  Using processor from Qwen/Qwen2-VL-2B-Instruct for correct chat template\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a68e777d6894466bb6314dfa153e8f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ qwen2-vl-2b loaded successfully\n",
            "\n",
            "All models loaded!\n"
          ]
        }
      ],
      "source": [
        "# Model configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"uground-2b\": \"osunlp/UGround-V1-2B\",\n",
        "    \"qwen2-vl-2b\": \"Qwen/Qwen2-VL-2B\",\n",
        "}\n",
        "\n",
        "# SGLang API endpoints\n",
        "API_ENDPOINTS = {\n",
        "    \"uground-2b\": \"http://localhost:8000/v1\",\n",
        "    \"qwen2-vl-2b\": \"http://localhost:8001/v1\",\n",
        "}\n",
        "\n",
        "# Processor configurations (for message formatting only)\n",
        "PROCESSOR_CONFIGS = {\n",
        "    \"uground-2b\": \"osunlp/UGround-V1-2B\",\n",
        "    \"qwen2-vl-2b\": \"Qwen/Qwen2-VL-2B-Instruct\",  # Use Instruct processor for correct chat template\n",
        "}\n",
        "\n",
        "# Initialize OpenAI clients for each model\n",
        "clients = {}\n",
        "processors = {}\n",
        "\n",
        "print(\"Setting up SGLang API clients...\")\n",
        "for key, endpoint in API_ENDPOINTS.items():\n",
        "    clients[key] = OpenAI(\n",
        "        base_url=endpoint,\n",
        "        api_key=\"EMPTY\"  # SGLang doesn't require a real API key\n",
        "    )\n",
        "    # Load processor for message formatting\n",
        "    processor_id = PROCESSOR_CONFIGS.get(key, MODEL_CONFIGS[key])\n",
        "    processors[key] = AutoProcessor.from_pretrained(processor_id)\n",
        "    print(f\"✓ {key} client ready at {endpoint}\")\n",
        "\n",
        "print(\"\\nAll API clients ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def image_to_base64(image: Image.Image) -> str:\n",
        "    \"\"\"Convert PIL Image to base64 data URL.\"\"\"\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "    return f\"data:image/png;base64,{img_str}\"\n",
        "\n",
        "\n",
        "def build_uground_messages(description: str, image_url: str):\n",
        "    \"\"\"\n",
        "    Build chat messages in OpenAI format for vision-language models.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": image_url}\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": f\"\"\"Find the coordinates of the element described below. Return ONLY the coordinates in the format (x, y) with no additional text.\n",
        "\n",
        "Description: {description}\n",
        "\n",
        "Coordinates:\"\"\"\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "def call_model_raw(\n",
        "    image: Image.Image,\n",
        "    query: str,\n",
        "    client: OpenAI,\n",
        "    model_name: str,\n",
        ") -> Tuple[str, int, int]:\n",
        "    \"\"\"\n",
        "    Send one screenshot + grounding query to SGLang API, return:\n",
        "    (raw_text_response, orig_width, orig_height).\n",
        "    \"\"\"\n",
        "    orig_w, orig_h = image.size\n",
        "    \n",
        "    # Convert image to base64\n",
        "    image_url = image_to_base64(image)\n",
        "    \n",
        "    # Build messages in OpenAI format\n",
        "    messages = build_uground_messages(query, image_url)\n",
        "    \n",
        "    try:\n",
        "        # Call SGLang API (OpenAI-compatible)\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            max_tokens=128,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        \n",
        "        reply = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"API call error: {e}\")\n",
        "        reply = \"\"\n",
        "    \n",
        "    return reply, orig_w, orig_h\n",
        "\n",
        "\n",
        "def parse_xy_from_string(text: str) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Extract (x, y) coordinates from a string.\n",
        "    Handles various formats:\n",
        "    - (x, y) - two coordinates\n",
        "    - (x1, y1, x2, y2) - four coordinates (bbox), computes center\n",
        "    - Multiple coordinate pairs - takes the first valid one\n",
        "    \"\"\"\n",
        "    if not text or not text.strip():\n",
        "        raise ValueError(f\"Empty text provided\")\n",
        "    \n",
        "    # Clean the text - remove extra whitespace and newlines\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Try to find coordinate patterns: (num, num) or (num, num, num, num)\n",
        "    # Pattern 1: (x, y) - two coordinates\n",
        "    pattern_two = re.findall(r\"\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\", text)\n",
        "    # Pattern 2: (x1, y1, x2, y2) - four coordinates (bbox)\n",
        "    pattern_four = re.findall(r\"\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\", text)\n",
        "    \n",
        "    # First try to find a simple (x, y) pair in [0, 1000) range\n",
        "    if pattern_two:\n",
        "        for match in pattern_two:\n",
        "            x, y = int(match[0]), int(match[1])\n",
        "            if 0 <= x < 1000 and 0 <= y < 1000:\n",
        "                return x, y\n",
        "    \n",
        "    # If we found a bbox format (x1, y1, x2, y2), compute center\n",
        "    if pattern_four:\n",
        "        for match in pattern_four:\n",
        "            x1, y1, x2, y2 = int(match[0]), int(match[1]), int(match[2]), int(match[3])\n",
        "            # Compute center\n",
        "            center_x = (x1 + x2) // 2\n",
        "            center_y = (y1 + y2) // 2\n",
        "            # If coordinates are in [0, 1000) range, use center\n",
        "            if 0 <= center_x < 1000 and 0 <= center_y < 1000:\n",
        "                return center_x, center_y\n",
        "            # If coordinates are in pixel space (> 1000), use first two if in range\n",
        "            elif 0 <= x1 < 1000 and 0 <= y1 < 1000:\n",
        "                return x1, y1\n",
        "    \n",
        "    # Fallback: try to extract just the first two numbers\n",
        "    numbers = re.findall(r'\\d+', text)\n",
        "    if len(numbers) >= 2:\n",
        "        x, y = int(numbers[0]), int(numbers[1])\n",
        "        # Clamp to [0, 1000) range\n",
        "        x = max(0, min(999, x))\n",
        "        y = max(0, min(999, y))\n",
        "        return x, y\n",
        "    \n",
        "    raise ValueError(f\"Could not parse valid coordinates from: {text[:200]!r}\")\n",
        "\n",
        "\n",
        "def scale_to_pixels(x_1000: int, y_1000: int, width: int, height: int) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Map model coordinates in [0,1000) to pixel coordinates of the original image.\n",
        "    \"\"\"\n",
        "    x_px = int(x_1000 / 1000 * width)\n",
        "    y_px = int(y_1000 / 1000 * height)\n",
        "    x_px = max(0, min(width - 1, x_px))\n",
        "    y_px = max(0, min(height - 1, y_px))\n",
        "    return x_px, y_px\n",
        "\n",
        "\n",
        "def predict_coordinates(\n",
        "    image: Image.Image,\n",
        "    query: str,\n",
        "    model_key: str,\n",
        ") -> Tuple[int, int, str]:\n",
        "    \"\"\"\n",
        "    Predict coordinates using the specified model via SGLang API.\n",
        "    Returns: (x_px, y_px, raw_response)\n",
        "    \"\"\"\n",
        "    client = clients[model_key]\n",
        "    model_name = MODEL_CONFIGS[model_key]\n",
        "    \n",
        "    reply, orig_w, orig_h = call_model_raw(image, query, client, model_name)\n",
        "    \n",
        "    # If reply is empty, return center as fallback\n",
        "    if not reply or reply.strip() == \"\":\n",
        "        return orig_w // 2, orig_h // 2, reply\n",
        "    \n",
        "    try:\n",
        "        x_1000, y_1000 = parse_xy_from_string(reply)\n",
        "        x_px, y_px = scale_to_pixels(x_1000, y_1000, orig_w, orig_h)\n",
        "        return x_px, y_px, reply\n",
        "    except ValueError as e:\n",
        "        # If parsing fails, return center of image as fallback\n",
        "        # Only print warning if reply is not empty (to avoid spam)\n",
        "        if reply.strip():\n",
        "            print(f\"Warning: Failed to parse coordinates from {model_key}: {e}, reply: {reply[:50]}\")\n",
        "        return orig_w // 2, orig_h // 2, reply\n",
        "\n",
        "\n",
        "def euclidean_distance(x1: int, y1: int, x2: int, y2: int) -> float:\n",
        "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
        "    return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "\n",
        "def bbox_to_center(bbox: List[int]) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Convert bounding box [x1, y1, x2, y2] to center coordinates (x, y).\n",
        "    \"\"\"\n",
        "    if len(bbox) != 4:\n",
        "        raise ValueError(f\"Bbox must have 4 elements, got {len(bbox)}\")\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    center_x = (x1 + x2) // 2\n",
        "    center_y = (y1 + y2) // 2\n",
        "    return center_x, center_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Click-100k Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Click-100k dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f0cc6bdbef04abf9b0baf5cf4d29030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "865bf20d06414b918673b2c69eccc1a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84a99d2c414e44f8a562f596d26d9d7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/58 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples in dataset: 101314\n",
            "Evaluating on 100 samples\n",
            "\n",
            "Dataset features: {'image_path': Value('string'), 'images': List(Image(mode=None, decode=True)), 'easyr1_prompt': Value('string'), 'bbox': List(Value('int64')), 'image_width': Value('int64'), 'image_height': Value('int64'), 'normalized_bbox': List(Value('float64'))}\n",
            "\n",
            "Sample keys: dict_keys(['image_path', 'images', 'easyr1_prompt', 'bbox', 'image_width', 'image_height', 'normalized_bbox'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading Click-100k dataset...\")\n",
        "dataset = load_dataset(\"mlfoundations/Click-100k\", split=\"train\")\n",
        "print(f\"Total samples in dataset: {len(dataset)}\")\n",
        "\n",
        "# Take first 100 samples\n",
        "eval_dataset = dataset.select(range(min(100, len(dataset))))\n",
        "print(f\"Evaluating on {len(eval_dataset)} samples\")\n",
        "print(f\"\\nDataset features: {eval_dataset.features}\")\n",
        "print(f\"\\nSample keys: {eval_dataset[0].keys()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing samples:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Processing samples:   1%|          | 1/100 [01:14<2:03:01, 74.56s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   2%|▏         | 2/100 [01:25<1:00:28, 37.03s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   3%|▎         | 3/100 [03:58<2:25:17, 89.87s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   4%|▍         | 4/100 [04:22<1:42:45, 64.23s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   5%|▌         | 5/100 [04:47<1:18:55, 49.85s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   6%|▌         | 6/100 [05:01<59:17, 37.85s/it]  Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   7%|▋         | 7/100 [06:27<1:23:06, 53.62s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   8%|▊         | 8/100 [07:25<1:24:04, 54.83s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:   9%|▉         | 9/100 [07:43<1:05:31, 43.20s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  10%|█         | 10/100 [07:58<52:10, 34.78s/it] Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  11%|█         | 11/100 [08:07<39:51, 26.87s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  12%|█▏        | 12/100 [10:14<1:23:44, 57.09s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Failed to parse coordinates from qwen2-vl-2b: Could not parse valid coordinates from: 'Based on the screenshot, here is my interpretation:\\n\\nThe user is playing chess on a website.\\n\\nThe user is currently on the home page of the website.\\n\\nThe user has not yet clicked the \"Play Now\" button'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing samples:  13%|█▎        | 13/100 [12:25<1:55:32, 79.69s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  14%|█▍        | 14/100 [12:33<1:23:07, 57.99s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  15%|█▌        | 15/100 [13:15<1:15:12, 53.09s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  16%|█▌        | 16/100 [13:44<1:04:05, 45.78s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  17%|█▋        | 17/100 [14:38<1:07:05, 48.50s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  18%|█▊        | 18/100 [15:12<1:00:07, 43.99s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  19%|█▉        | 19/100 [15:38<52:15, 38.71s/it]  Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  20%|██        | 20/100 [18:06<1:35:21, 71.52s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  21%|██        | 21/100 [18:22<1:12:00, 54.70s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  22%|██▏       | 22/100 [18:32<53:31, 41.18s/it]  Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  23%|██▎       | 23/100 [20:01<1:11:23, 55.64s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  24%|██▍       | 24/100 [22:48<1:52:45, 89.01s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  25%|██▌       | 25/100 [23:29<1:33:28, 74.78s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  26%|██▌       | 26/100 [26:20<2:07:48, 103.63s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  27%|██▋       | 27/100 [27:54<2:02:37, 100.78s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  28%|██▊       | 28/100 [28:19<1:33:28, 77.90s/it] Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  29%|██▉       | 29/100 [28:37<1:11:01, 60.03s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  30%|███       | 30/100 [31:11<1:43:00, 88.29s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  31%|███       | 31/100 [31:25<1:15:43, 65.84s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  32%|███▏      | 32/100 [31:33<54:51, 48.41s/it]  Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  33%|███▎      | 33/100 [32:22<54:15, 48.59s/it]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "Processing samples:  33%|███▎      | 33/100 [33:21<1:07:42, 60.64s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_key \u001b[38;5;129;01min\u001b[39;00m MODEL_CONFIGS.keys():\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         pred_x, pred_y, raw_response = \u001b[43mpredict_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m         distance = euclidean_distance(pred_x, pred_y, gt_x, gt_y)\n\u001b[32m     52\u001b[39m         results[model_key].append({\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msample_idx\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m     54\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mraw_response\u001b[39m\u001b[33m\"\u001b[39m: raw_response,\n\u001b[32m     62\u001b[39m         })\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mpredict_coordinates\u001b[39m\u001b[34m(image, query, model_key)\u001b[39m\n\u001b[32m    148\u001b[39m processor = processors[model_key]\n\u001b[32m    149\u001b[39m model = models[model_key]\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m reply, orig_w, orig_h = \u001b[43mcall_model_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    154\u001b[39m     x_1000, y_1000 = parse_xy_from_string(reply)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mcall_model_raw\u001b[39m\u001b[34m(image, query, processor, model)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Remove cache-related keys that might cause issues with Qwen2-VL\u001b[39;00m\n\u001b[32m     58\u001b[39m     clean_inputs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \n\u001b[32m     59\u001b[39m                    \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclean_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Decode only the newly generated tokens\u001b[39;00m\n\u001b[32m     70\u001b[39m gen_ids = outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1356\u001b[39m, in \u001b[36mQwen2VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1351\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m   1352\u001b[39m output_hidden_states = (\n\u001b[32m   1353\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1354\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1374\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1375\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1187\u001b[39m, in \u001b[36mQwen2VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1184\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.get_input_embeddings()(input_ids)\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m     image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1188\u001b[39m     image_embeds = torch.cat(image_embeds, dim=\u001b[32m0\u001b[39m).to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m   1189\u001b[39m     image_mask, _ = \u001b[38;5;28mself\u001b[39m.get_placeholder_mask(\n\u001b[32m   1190\u001b[39m         input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n\u001b[32m   1191\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:1103\u001b[39m, in \u001b[36mQwen2VLModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, image_grid_thw)\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1094\u001b[39m \u001b[33;03mEncodes images into continuous embeddings that can be forwarded to the language model.\u001b[39;00m\n\u001b[32m   1095\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1100\u001b[39m \u001b[33;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[32m   1101\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1102\u001b[39m pixel_values = pixel_values.type(\u001b[38;5;28mself\u001b[39m.visual.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m split_sizes = (image_grid_thw.prod(-\u001b[32m1\u001b[39m) // \u001b[38;5;28mself\u001b[39m.visual.spatial_merge_size**\u001b[32m2\u001b[39m).tolist()\n\u001b[32m   1105\u001b[39m image_embeds = torch.split(image_embeds, split_sizes)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:729\u001b[39m, in \u001b[36mQwen2VisionTransformerPretrainedModel.forward\u001b[39m\u001b[34m(self, hidden_states, grid_thw, **kwargs)\u001b[39m\n\u001b[32m    726\u001b[39m cu_seqlens = F.pad(cu_seqlens, (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), value=\u001b[32m0\u001b[39m)\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     hidden_states = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.merger(hidden_states)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:422\u001b[39m, in \u001b[36mQwen2VLVisionBlock.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    415\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    416\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    420\u001b[39m     **kwargs,\n\u001b[32m    421\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(hidden_states))\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:379\u001b[39m, in \u001b[36mVisionAttention.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Other implementations: Process each chunk separately\u001b[39;00m\n\u001b[32m    378\u001b[39m     lengths = cu_seqlens[\u001b[32m1\u001b[39m:] - cu_seqlens[:-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     splits = \u001b[43m[\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    383\u001b[39m     attn_outputs = [\n\u001b[32m    384\u001b[39m         attention_interface(\n\u001b[32m    385\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m q, k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*splits)\n\u001b[32m    396\u001b[39m     ]\n\u001b[32m    397\u001b[39m     attn_output = torch.cat(attn_outputs, dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ecua/lib/python3.11/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py:380\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Other implementations: Process each chunk separately\u001b[39;00m\n\u001b[32m    378\u001b[39m     lengths = cu_seqlens[\u001b[32m1\u001b[39m:] - cu_seqlens[:-\u001b[32m1\u001b[39m]\n\u001b[32m    379\u001b[39m     splits = [\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m         torch.split(tensor, \u001b[43mlengths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m (query_states, key_states, value_states)\n\u001b[32m    381\u001b[39m     ]\n\u001b[32m    383\u001b[39m     attn_outputs = [\n\u001b[32m    384\u001b[39m         attention_interface(\n\u001b[32m    385\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m q, k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*splits)\n\u001b[32m    396\u001b[39m     ]\n\u001b[32m    397\u001b[39m     attn_output = torch.cat(attn_outputs, dim=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Initialize results storage\n",
        "results = {\n",
        "    \"uground-2b\": [],\n",
        "    \"qwen2-vl-2b\": []\n",
        "}\n",
        "\n",
        "# Process each sample\n",
        "print(\"Running evaluation...\")\n",
        "for idx in tqdm(range(len(eval_dataset)), desc=\"Processing samples\"):\n",
        "    sample = eval_dataset[idx]\n",
        "    \n",
        "    # Extract image from dataset - images is a list, take the first one\n",
        "    images_list = sample.get(\"images\")\n",
        "    if images_list is None or len(images_list) == 0:\n",
        "        print(f\"Skipping sample {idx}: no images found\")\n",
        "        continue\n",
        "    \n",
        "    # Get the first image from the list\n",
        "    image = images_list[0]\n",
        "    if not isinstance(image, Image.Image):\n",
        "        # If it's not already a PIL Image, try to convert\n",
        "        if hasattr(image, \"convert\"):\n",
        "            image = image.convert(\"RGB\")\n",
        "        else:\n",
        "            print(f\"Skipping sample {idx}: cannot process image format\")\n",
        "            continue\n",
        "    \n",
        "    # Extract query\n",
        "    query = sample.get(\"easyr1_prompt\")\n",
        "    if query is None:\n",
        "        print(f\"Skipping sample {idx}: missing query\")\n",
        "        continue\n",
        "    \n",
        "    # Extract bbox and convert to center coordinates\n",
        "    bbox = sample.get(\"bbox\")\n",
        "    if bbox is None or len(bbox) != 4:\n",
        "        print(f\"Skipping sample {idx}: invalid bbox {bbox}\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        gt_x, gt_y = bbox_to_center(bbox)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping sample {idx}: error converting bbox: {e}\")\n",
        "        continue\n",
        "    \n",
        "    # Evaluate both models\n",
        "    for model_key in MODEL_CONFIGS.keys():\n",
        "        try:\n",
        "            pred_x, pred_y, raw_response = predict_coordinates(image, query, model_key)\n",
        "            distance = euclidean_distance(pred_x, pred_y, gt_x, gt_y)\n",
        "            \n",
        "            results[model_key].append({\n",
        "                \"sample_idx\": idx,\n",
        "                \"query\": query,\n",
        "                \"bbox\": bbox,\n",
        "                \"gt_x\": gt_x,\n",
        "                \"gt_y\": gt_y,\n",
        "                \"pred_x\": pred_x,\n",
        "                \"pred_y\": pred_y,\n",
        "                \"distance\": distance,\n",
        "                \"raw_response\": raw_response,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {idx} with {model_key}: {e}\")\n",
        "            results[model_key].append({\n",
        "                \"sample_idx\": idx,\n",
        "                \"query\": query,\n",
        "                \"bbox\": bbox,\n",
        "                \"gt_x\": gt_x,\n",
        "                \"gt_y\": gt_y,\n",
        "                \"pred_x\": None,\n",
        "                \"pred_y\": None,\n",
        "                \"distance\": float('inf'),\n",
        "                \"raw_response\": str(e),\n",
        "                \"error\": True,\n",
        "            })\n",
        "\n",
        "print(\"\\nEvaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Calculate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(results: List[Dict]) -> Dict:\n",
        "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
        "    distances = [r[\"distance\"] for r in results if r.get(\"distance\") != float('inf')]\n",
        "    \n",
        "    if not distances:\n",
        "        return {\n",
        "            \"mean_distance\": float('inf'),\n",
        "            \"median_distance\": float('inf'),\n",
        "            \"std_distance\": float('inf'),\n",
        "            \"accuracy_5px\": 0.0,\n",
        "            \"accuracy_10px\": 0.0,\n",
        "            \"accuracy_20px\": 0.0,\n",
        "            \"accuracy_50px\": 0.0,\n",
        "            \"total_samples\": len(results),\n",
        "            \"successful_samples\": 0,\n",
        "        }\n",
        "    \n",
        "    distances = np.array(distances)\n",
        "    \n",
        "    # Calculate accuracy at different thresholds\n",
        "    thresholds = [5, 10, 20, 50]\n",
        "    accuracies = {}\n",
        "    for threshold in thresholds:\n",
        "        accuracies[f\"accuracy_{threshold}px\"] = np.mean(distances <= threshold) * 100\n",
        "    \n",
        "    return {\n",
        "        \"mean_distance\": float(np.mean(distances)),\n",
        "        \"median_distance\": float(np.median(distances)),\n",
        "        \"std_distance\": float(np.std(distances)),\n",
        "        **accuracies,\n",
        "        \"total_samples\": len(results),\n",
        "        \"successful_samples\": len(distances),\n",
        "    }\n",
        "\n",
        "# Calculate metrics for both models\n",
        "metrics = {}\n",
        "for model_key in MODEL_CONFIGS.keys():\n",
        "    metrics[model_key] = calculate_metrics(results[model_key])\n",
        "    print(f\"\\n{model_key.upper()} Metrics:\")\n",
        "    print(f\"  Mean Distance Error: {metrics[model_key]['mean_distance']:.2f} px\")\n",
        "    print(f\"  Median Distance Error: {metrics[model_key]['median_distance']:.2f} px\")\n",
        "    print(f\"  Std Distance Error: {metrics[model_key]['std_distance']:.2f} px\")\n",
        "    print(f\"  Accuracy @ 5px: {metrics[model_key]['accuracy_5px']:.2f}%\")\n",
        "    print(f\"  Accuracy @ 10px: {metrics[model_key]['accuracy_10px']:.2f}%\")\n",
        "    print(f\"  Accuracy @ 20px: {metrics[model_key]['accuracy_20px']:.2f}%\")\n",
        "    print(f\"  Accuracy @ 50px: {metrics[model_key]['accuracy_50px']:.2f}%\")\n",
        "    print(f\"  Successful Predictions: {metrics[model_key]['successful_samples']}/{metrics[model_key]['total_samples']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for model_key in MODEL_CONFIGS.keys():\n",
        "    m = metrics[model_key]\n",
        "    comparison_data.append({\n",
        "        \"Model\": model_key,\n",
        "        \"Mean Distance (px)\": f\"{m['mean_distance']:.2f}\",\n",
        "        \"Median Distance (px)\": f\"{m['median_distance']:.2f}\",\n",
        "        \"Std Distance (px)\": f\"{m['std_distance']:.2f}\",\n",
        "        \"Accuracy @ 5px (%)\": f\"{m['accuracy_5px']:.2f}\",\n",
        "        \"Accuracy @ 10px (%)\": f\"{m['accuracy_10px']:.2f}\",\n",
        "        \"Accuracy @ 20px (%)\": f\"{m['accuracy_20px']:.2f}\",\n",
        "        \"Accuracy @ 50px (%)\": f\"{m['accuracy_50px']:.2f}\",\n",
        "        \"Success Rate\": f\"{m['successful_samples']}/{m['total_samples']}\",\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(df_comparison.to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distance distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for idx, model_key in enumerate(MODEL_CONFIGS.keys()):\n",
        "    distances = [r[\"distance\"] for r in results[model_key] \n",
        "                 if r.get(\"distance\") != float('inf')]\n",
        "    \n",
        "    if distances:\n",
        "        axes[idx].hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[idx].axvline(np.mean(distances), color='red', linestyle='--', \n",
        "                         label=f'Mean: {np.mean(distances):.2f}px')\n",
        "        axes[idx].axvline(np.median(distances), color='green', linestyle='--', \n",
        "                         label=f'Median: {np.median(distances):.2f}px')\n",
        "        axes[idx].set_xlabel('Distance Error (px)')\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].set_title(f'{model_key.upper()} - Distance Error Distribution')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, 'No valid predictions', \n",
        "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
        "        axes[idx].set_title(f'{model_key.upper()} - No Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot accuracy comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "thresholds = [5, 10, 20, 50]\n",
        "x = np.arange(len(thresholds))\n",
        "width = 0.35\n",
        "\n",
        "uground_accs = [metrics[\"uground-2b\"][f\"accuracy_{t}px\"] for t in thresholds]\n",
        "qwen_accs = [metrics[\"qwen2-vl-2b\"][f\"accuracy_{t}px\"] for t in thresholds]\n",
        "\n",
        "bars1 = ax.bar(x - width/2, uground_accs, width, label='UGround-V1-2B', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, qwen_accs, width, label='Qwen2-VL-2B', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Distance Threshold (px)')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Accuracy Comparison at Different Distance Thresholds')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{t}px' for t in thresholds])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Sample Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some example predictions\n",
        "num_examples = min(5, len(eval_dataset))\n",
        "print(f\"\\nShowing {num_examples} example predictions:\\n\")\n",
        "\n",
        "for idx in range(num_examples):\n",
        "    sample = eval_dataset[idx]\n",
        "    images_list = sample.get(\"images\")\n",
        "    query = sample.get(\"easyr1_prompt\")\n",
        "    bbox = sample.get(\"bbox\")\n",
        "    \n",
        "    if images_list is None or len(images_list) == 0 or query is None or bbox is None:\n",
        "        continue\n",
        "    \n",
        "    image = images_list[0]\n",
        "    if not isinstance(image, Image.Image):\n",
        "        if hasattr(image, \"convert\"):\n",
        "            image = image.convert(\"RGB\")\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    try:\n",
        "        gt_x, gt_y = bbox_to_center(bbox)\n",
        "    except:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Example {idx + 1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Bbox: {bbox}\")\n",
        "    print(f\"Ground Truth Center: ({gt_x}, {gt_y})\")\n",
        "    \n",
        "    for model_key in MODEL_CONFIGS.keys():\n",
        "        # Find the result for this sample index\n",
        "        result = None\n",
        "        for r in results[model_key]:\n",
        "            if r.get(\"sample_idx\") == idx:\n",
        "                result = r\n",
        "                break\n",
        "        \n",
        "        if result is None:\n",
        "            print(f\"\\n{model_key}: No result found\")\n",
        "            continue\n",
        "            \n",
        "        if result.get(\"error\"):\n",
        "            print(f\"\\n{model_key}: ERROR - {result['raw_response']}\")\n",
        "        else:\n",
        "            print(f\"\\n{model_key}:\")\n",
        "            print(f\"  Ground Truth: ({result['gt_x']}, {result['gt_y']})\")\n",
        "            print(f\"  Predicted: ({result['pred_x']}, {result['pred_y']})\")\n",
        "            print(f\"  Distance Error: {result['distance']:.2f} px\")\n",
        "            print(f\"  Raw Response: {result['raw_response']}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ecua",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
